\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 
\usepackage{url}
\usepackage{amssymb, amsmath, amsthm}
\usepackage{float}

\newcommand{\leftcodemean}{{\langle\;\!\!\!\langle}}
\newcommand{\rightcodemean}{{\rangle\;\!\!\!\rangle}}
%\newcommand{\codemean}[1]{\leftcodemean #1 \rightcodemean}
%\newcommand{\expmean}[1]{\lvert #1 \rvert}
\newcommand{\commean}[1]{\lvert #1 \rvert}
\renewcommand{\emptyset}{\{\}}
\newcommand{\inl}[1]{{\bf inl}\;#1}
\newcommand{\inr}[1]{{\bf inr}\;#1}
\newcommand{\sumcase}[5]{{\bf sumcase}\ #1\ {\bf of}\ \inl{#2} \Rightarrow #3 \,{\bf |}\, \inr{#4} \Rightarrow #5}
\newcommand{\inj}{\iota}
\newcommand{\tymean}[1]{\llbracket #1\rrbracket}
\newcommand{\expmean}[1]{\llbracket #1\rrbracket}
\newcommand{\nsqsubseteq}{\not\sqsubseteq}
\newcommand{\listcase}[5]{{\bf listcase}\ #1\ {\bf of}\ {\bf nil} \Rightarrow #2 \,{\bf |}\, {\bf cons}(#3, #4) \Rightarrow #5}
\newcommand{\listcaseml}[5]{\begin{array}[t]{@{}l}%
                            {\bf listcase}\ #1\ {\bf of} \\%
                            \;\; {\bf nil} \Rightarrow #2 \\%
                            {\bf |}\, {\bf cons}(#3, #4) \Rightarrow #5%
                            \end{array}}
\newcommand{\envmean}[1]{\llbracket #1 \rrbracket}
\newcommand{\cross}{\times}


\title{ Research Plan:  Fundamental Parallel Algorithms  on  a Thousand-Core Machine (TODO: better name)} 
\author{Aapo Kyrola}


 
\begin{document} 
 \maketitle

\section{Abstract}

\section{Research Question}

How to execute efficiently classical divide- and conquer parallel algorithms\footnote{And parallel algorithms
that have a divide- and conquer-based subroutine such as scan or reduce.} on a shared-memory system with
hundreds of cores? How should such algorithms be programmed? Is there a "sweet-spot" between multi-core
parallel programs and distributed algorithms?



We think this research is particularly relevant, because it is likely that future servers and even desktops will look more like
Blacklight today:  huge amount of processors with non-uniform access to shared memory (NUMA).

\section{Working Hypothesis}

\begin{enumerate}

\item Current parallel extensions to C, CILK and OpenMP will not scale sufficiently on Blacklight-scale.

\item For efficiency, algorithms or algorithm runtime needs to be aware of cpu-affinity of data, i.e location of data.

\item For good performance, one must bypass or manipulate the cache-coherence protocols.

\end{enumerate}

\section{Resources}

\begin{itemize} 
\item Uniform Parallel C: UPC  \url{http://upc.gwu.edu/}
\item Global Reference Unit  (GRU) SDK for SGI Altix \url{http://techpubs.sgi.com/library/manuals/5000/007-5668-002/pdf/007-5668-002.pdf}
\item TeraSort benchmar \url{http://sortbenchmark.org/}
\end{itemize}

 
\section{Related work}

\subsection{Tera-sort benchmark work}

See OSDI 2011?

\begin{itemize}
\item TritonSort paper: \url{http://sortbenchmark.org/2011_06_tritonsort.pdf}
\end{itemize}

\section{Plan Outline}

\subsection{Work done so far}

In Spring 2011, we did some micro-benchmarks to study the memory access performance patterns of BlackLight.
We found out curious behavior, which we could explain by the cache coherence protocol. This highlighted the need
to keep track of thread-affinity of data.  The study is attached as an appendix.

\subsection{Algorithms / Problems}

I suggest choosing four basic problems from the PBBS suite for an initial study set. These are all
fundamental algorithms, and such that are not easy to run in a distributed system, but scale quite well
on a multicore machine with a handful of cores. They are all divide-and-conquer problems.

\begin{enumerate}
\item Sort (strings)
\item Scan (prefix-sum)
\item 1-nearest neighbor
\item Suffix array.

\end{enumerate}

\subsection{Plan of attack}

\begin{enumerate}

\item Try to get selected PBBS algorithms run on BlackLight, up to 256 cores.


\item Analyze their scalability, efficiency on moderate size data.

\item Assuming the scalability is poor, start experimenting and working on the hypothesis.
 
\item Plan next steps. 

\end{enumerate} 

\subsection{Frameworks to evaluate}

\begin{itemize}
\item CILK / OpenMP
\item Sequioa++ (Stanford)
\item Harsha's scheduler
\end{itemize}
 \newpage
 
 
 \section{Appendix}
 
 \end{document}
 
